{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algoritmo de Gradiente de boosting\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv('malicius_dataset.csv')\n",
    "\n",
    "# Mostrar información inicial del dataset\n",
    "print(\"Información inicial del dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "# Convertir la columna 'type' a valores numéricos (0 para benigno, 1 para phishing)\n",
    "label_encoder = LabelEncoder()\n",
    "df['type'] = label_encoder.fit_transform(df['type'])\n",
    "\n",
    "# Definir características (X) y variable objetivo (y)\n",
    "X = df['url']  # Características: URL\n",
    "y = df['type']  # Variable objetivo: tipo (benigno o phishing)\n",
    "\n",
    "# Convertir las URLs en características numéricas usando técnicas de vectorización\n",
    "X_vectorized = pd.DataFrame()\n",
    "\n",
    "# Contar la longitud de la URL\n",
    "X_vectorized['largo_url'] = X.apply(len)\n",
    "\n",
    "# Contar la cantidad de dígitos en la URL\n",
    "X_vectorized['cantidad_digitos'] = X.apply(lambda url: sum(c.isdigit() for c in url))\n",
    "\n",
    "# Contar la cantidad de caracteres especiales en la URL\n",
    "X_vectorized['cantidad_caracteres_especiales'] = X.apply(lambda url: sum(c in string.punctuation for c in url))\n",
    "\n",
    "# Contar vocales y consonantes\n",
    "vowels = 'aeiou'\n",
    "X_vectorized['cantidad_vocales'] = X.apply(lambda url: sum(c.lower() in vowels for c in url))\n",
    "X_vectorized['cantidad_consonantes'] = X.apply(lambda url: sum(c.isalpha() and c.lower() not in vowels for c in url))\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de Gradient Boosting\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones sobre el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "puntaje = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "reporte = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Puntaje: {puntaje:.2f}')\n",
    "print('Matriz de Confusion:')\n",
    "print(confusion)\n",
    "print('Reporte:')\n",
    "print(reporte)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
