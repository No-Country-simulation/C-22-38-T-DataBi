{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza del Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datas\n",
    "data1= pd.read_csv('phishing_site_urls.csv') # kaggle\n",
    "data2= pd.read_csv('URL dataset.csv') #data phishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549346 entries, 0 to 549345\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   URL     549346 non-null  object\n",
      " 1   Label   549346 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 450176 entries, 0 to 450175\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   url     450176 non-null  object\n",
      " 1   type    450176 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos informacion rapida de los datas\n",
    "data1.info()\n",
    "data2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en data1:\n",
      " URL      0\n",
      "Label    0\n",
      "dtype: int64\n",
      "\n",
      "Valores nulos en data2:\n",
      " url     0\n",
      "type    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verificar valores nulos en cada columna \n",
    "valores_nulos_data1 = data1.isnull().sum() \n",
    "valores_nulos_data2 = data2.isnull().sum() \n",
    "print(\"Valores nulos en data1:\\n\", valores_nulos_data1) \n",
    "print(\"\\nValores nulos en data2:\\n\", valores_nulos_data2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marisol\\AppData\\Local\\Temp\\ipykernel_3864\\2263928669.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  espacios_vacios_data1 = data1.applymap(lambda x: isinstance(x, str) and x.strip() == '').sum()\n",
      "C:\\Users\\Marisol\\AppData\\Local\\Temp\\ipykernel_3864\\2263928669.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  espacios_vacios_data2 = data2.applymap(lambda x: isinstance(x, str) and x.strip() == '').sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Espacios vacíos en data1:\n",
      " URL      0\n",
      "Label    0\n",
      "dtype: int64\n",
      "\n",
      "Espacios vacíos en data2:\n",
      " url     0\n",
      "type    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verificar espacios vacíos en cada columna \n",
    "espacios_vacios_data1 = data1.applymap(lambda x: isinstance(x, str) and x.strip() == '').sum() \n",
    "espacios_vacios_data2 = data2.applymap(lambda x: isinstance(x, str) and x.strip() == '').sum()\n",
    "print(\"\\nEspacios vacíos en data1:\\n\", espacios_vacios_data1) \n",
    "print(\"\\nEspacios vacíos en data2:\\n\", espacios_vacios_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad' 'good']\n",
      "['legitimate' 'phishing']\n"
     ]
    }
   ],
   "source": [
    "# Exploramos los valores unicos de las columnas 'Label' y 'type'\n",
    "valores = data1['Label'].unique()\n",
    "print(valores)\n",
    "\n",
    "valores = data2['type'].unique()\n",
    "print(valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 42150 filas duplicadas\n",
      "Se ha eliminado 42150 filas\n",
      "Datas sin filas duplicadas\n",
      "Hay 0 filas duplicadas\n",
      "Se ha eliminado 0 filas\n",
      "Datas sin filas duplicadas\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos filas duplicadas \n",
    "for df in [data1, data2]:\n",
    "    duplicados= df.duplicated().sum()\n",
    "    print(f\"Hay {duplicados} filas duplicadas\")\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    eliminado=duplicados\n",
    "    print(f\"Se ha eliminado {eliminado} filas\")\n",
    "    \n",
    "    print(f\"Datas sin filas duplicadas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenamos datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las datas han sido unidas\n",
      "Index(['url', 'status'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#renombrar columnas \n",
    "data1.rename(columns={'URL': 'url', 'Label': 'status'}, inplace=True)\n",
    "data2.rename(columns={'type': 'status'}, inplace=True)\n",
    "\n",
    "\n",
    "#unir datas\n",
    "datas=pd.concat([data1,data2], ignore_index=True)\n",
    "\n",
    "#actualizamos las etiquetas del data1\n",
    "datas['status']=datas['status'].replace({'bad': '-1', 'good': '1', 'phishing': '-1', 'legitimate':'1'})\n",
    "print(\"Las datas han sido unidas\")\n",
    "print(datas.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 957372 entries, 0 to 957371\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   url     957372 non-null  object\n",
      " 1   status  957372 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 14.6+ MB\n"
     ]
    }
   ],
   "source": [
    "datas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status\n",
      "1     738635\n",
      "-1    218737\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# La columna status tiene como valores de 1=legitimate, -1=phishing\n",
    "conteo_datas= datas['status'].value_counts()\n",
    "print(conteo_datas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      url status  has_https  \\\n",
      "0       nobell.it/70ffb52d079109dca5664cce6f317373782/...     -1          0   \n",
      "1       www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...     -1          0   \n",
      "2       serviciosbys.com/paypal.cgi.bin.get-into.herf....     -1          0   \n",
      "3       mail.printakid.com/www.online.americanexpress....     -1          0   \n",
      "4       thewhiskeydregs.com/wp-content/themes/widescre...     -1          0   \n",
      "...                                                   ...    ...        ...   \n",
      "957367        http://ecct-it.com/docmmmnn/aptgd/index.php     -1          0   \n",
      "957368  http://faboleena.com/js/infortis/jquery/plugin...     -1          0   \n",
      "957369  http://faboleena.com/js/infortis/jquery/plugin...     -1          0   \n",
      "957370                             http://atualizapj.com/     -1          0   \n",
      "957371  http://writeassociate.com/test/Portal/inicio/I...     -1          0   \n",
      "\n",
      "        has_http  has_ftp  has_www  \n",
      "0              0        0        0  \n",
      "1              0        0        1  \n",
      "2              0        0        0  \n",
      "3              0        0        0  \n",
      "4              0        0        0  \n",
      "...          ...      ...      ...  \n",
      "957367         1        0        0  \n",
      "957368         1        0        0  \n",
      "957369         1        0        0  \n",
      "957370         1        0        0  \n",
      "957371         1        0        0  \n",
      "\n",
      "[957372 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Funcion para verificar si la url usa ciertos prefijos\n",
    "def has_prefix(url, prefix): \n",
    "    return 1 if url.startswith(prefix) else 0 \n",
    "# Crear columnas para cada prefijo en el DataFrame \n",
    "datas['has_https'] = datas['url'].apply(lambda x: has_prefix(x, 'https://')) \n",
    "datas['has_http'] = datas['url'].apply(lambda x: has_prefix(x, 'http://')) \n",
    "datas['has_ftp'] = datas['url'].apply(lambda x: has_prefix(x, 'ftp://')) \n",
    "datas['has_www'] = datas['url'].apply(lambda x: has_prefix(x, 'www.'))\n",
    "print(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma de filas que empiezan con www. es:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Función para estandarizar las urls # funcion para eliminar espacios blanco al inicio y final de las urls\n",
    "def clean_url(url): \n",
    "    url= re.sub(r'\\s+', '', str(url)) #eliminar espacios blancos al inicio y final de la url\n",
    "    prefixes=['https://', 'http://', 'ftp://', 'www.']\n",
    "    \n",
    "    for prefix in prefixes: \n",
    "        while url.startswith(prefix):\n",
    "            url=url[len(prefix):]\n",
    "    return url\n",
    "#aplicar la funcion para limpiar las url\n",
    "datas['url']=datas['url'].apply(clean_url)\n",
    "\n",
    "#funcion para verificar si hay urls q comiencen con www\n",
    "def has_www(df):\n",
    "    return df[df['url'].str.startswith('www.')].shape[0]\n",
    "\n",
    "#bucle para eliminar 'www.' hasta q no queden url q empiecen cn ese prefijo\n",
    "while has_www(datas)>0:\n",
    "    datas['url']=datas['url'].apply(clean_url)\n",
    "#verificar q todas las url han sido eliminadas\n",
    "suma_filas_www= has_www(datas)\n",
    "print(\"la suma de filas que empiezan con www. es:\")\n",
    "print(suma_filas_www)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 6536 filas duplicadas\n",
      "Se ha eliminado 6536 filas\n",
      "Datas sin filas duplicadas\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos filas duplicadas en 'datas'\n",
    "for df in [datas]:\n",
    "    duplicados= df.duplicated().sum()\n",
    "    print(f\"Hay {duplicados} filas duplicadas\")\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    eliminado=duplicados\n",
    "    print(f\"Se ha eliminado {eliminado} filas\")\n",
    "    print(f\"Datas sin filas duplicadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marisol\\AppData\\Local\\Temp\\ipykernel_3864\\1804463928.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datas[col_name]=datas['url'].apply(lambda x: x.count(char))\n",
      "C:\\Users\\Marisol\\AppData\\Local\\Temp\\ipykernel_3864\\1804463928.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datas['num_subdomains'] = datas['url'].apply(lambda x: x.count('.'))\n",
      "C:\\Users\\Marisol\\AppData\\Local\\Temp\\ipykernel_3864\\1804463928.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datas['has_ip'] = datas['url'].apply(starts_with_ip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  has_https  has_http  \\\n",
      "0  nobell.it/70ffb52d079109dca5664cce6f317373782/...          0         0   \n",
      "1  dghjdgf.com/paypal.co.uk/cycgi-bin/webscrcmd=_...          0         0   \n",
      "2  serviciosbys.com/paypal.cgi.bin.get-into.herf....          0         0   \n",
      "3  mail.printakid.com/www.online.americanexpress....          0         0   \n",
      "4  thewhiskeydregs.com/wp-content/themes/widescre...          0         0   \n",
      "\n",
      "   has_ftp  has_www  repeated_chars  space_count  url_lenght  \\\n",
      "0        0        0              10            0         225   \n",
      "1        0        1               0            0          77   \n",
      "2        0        0               3            0         177   \n",
      "3        0        0               3            0          60   \n",
      "4        0        0               3            0         116   \n",
      "\n",
      "   num_special_chars  count_a_lower  ...  count_,  count_.  count_<  count_>  \\\n",
      "0                 32             10  ...        0        6        0        0   \n",
      "1                 14              4  ...        0        4        0        0   \n",
      "2                 19              7  ...        0        7        0        0   \n",
      "3                  8              4  ...        0        6        0        0   \n",
      "4                 13              0  ...        0        1        0        0   \n",
      "\n",
      "   count_?  count_/  count_\\  num_subdomains  has_ip  status  \n",
      "0        1       10        0               6       0      -1  \n",
      "1        0        4        0               4       0      -1  \n",
      "2        0       11        0               7       0      -1  \n",
      "3        0        2        0               6       0      -1  \n",
      "4        1       10        0               1       0      -1  \n",
      "\n",
      "[5 rows x 103 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Se añade columnas\n",
    "#definir el alfabeto y otros caracteres\n",
    "alfabeto_minuscula= string.ascii_lowercase\n",
    "alfabeto_mayuscula=string.ascii_uppercase\n",
    "caracteres_str=\"!@#$%^&*()_+-=[]{}|;:',.<>?/\\\\\"\n",
    "caracteres=list(caracteres_str)\n",
    "numeros=\"0123456789\"\n",
    "\n",
    "#funcion para contar las concurrencias de cada caracter\n",
    "def count_chars(url,chars):\n",
    "    return{chars:url.count(char)for char in chars}\n",
    "\n",
    "#funcion para contar caracteres repetidos\n",
    "def caracteres_repet(url):\n",
    "    counts={}\n",
    "    for char in url:\n",
    "        if char in counts:\n",
    "            counts[char] +=1\n",
    "        else:\n",
    "            counts[char]=1\n",
    "        return sum(count for count in counts.values() if count>1)\n",
    "\n",
    "#funcion para contar digitos \n",
    "def cuenta_digitos_rep(url):\n",
    "    digitos = {} \n",
    "    for char in url: \n",
    "        if char.isdigit():\n",
    "            if char in digitos: \n",
    "                digitos[char] += 1 \n",
    "            else: \n",
    "                digitos[char] = 1 \n",
    "    return digitos \n",
    "\n",
    "# Añadir columna para caracteres repetidos consecutivos (ejemplo: aa, bb, //)\n",
    "def caracteres_repet(url): \n",
    "    count = 0 \n",
    "    for i in range(1, len(url)): \n",
    "        if url[i] == url[i-1]: \n",
    "            count += 1 \n",
    "    return count\n",
    "datas['repeated_chars'] = datas['url'].apply(caracteres_repet) \n",
    "\n",
    "# Añadir columna para espacios vacíos \n",
    "#funcion para contar los espacios en una url\n",
    "def count_spaces(url):\n",
    "    return url.count (' ')\n",
    "datas['space_count'] = datas['url'].apply(count_spaces)\n",
    "\n",
    "\n",
    "#añadir columna longitud de la url\n",
    "datas['url_lenght']=datas['url'].apply(len)\n",
    "\n",
    "#añadir numero de caracteres especiales\n",
    "datas['num_special_chars']=datas['url'].apply(lambda x: sum(x.count(c) for c in caracteres))\n",
    "#añadir columnas para cada letra en minuscula\n",
    "for char in alfabeto_minuscula:\n",
    "    datas[f'count_{char}_lower'] = datas['url'].apply(lambda x: x.count(char))\n",
    "#añadir columnas para cada letra en mayuscula\n",
    "for char in alfabeto_mayuscula:\n",
    "    datas[f'count_{char}_upper'] = datas['url'].apply(lambda x: x.count(char))\n",
    "\n",
    "#añadir columnas para cada numeor\n",
    "for num in numeros:\n",
    "    datas[num]=datas['url'].apply(lambda x: x.count(num))\n",
    "\n",
    "#añadir columnas para caracteres\n",
    "for char in caracteres:\n",
    "    col_name=f\"count_{char}\"\n",
    "    datas[col_name]=datas['url'].apply(lambda x: x.count(char))\n",
    "\n",
    "\n",
    "\n",
    "# Añadir columna de número de subdominios \n",
    "#funcion para contar los subdominios en una url\n",
    "def count_subdomains(url):\n",
    "    url=re.sub(r'^(http://|https://|ftp://|www\\.)', '', url)\n",
    "    return url.count('.')\n",
    "datas['num_subdomains'] = datas['url'].apply(lambda x: x.count('.')) \n",
    "\n",
    "\n",
    "\n",
    "#añade columna has_ip\n",
    "# Función para verificar si una URL comienza con una dirección IP \n",
    "def starts_with_ip(url): \n",
    "    # Expresión regular para encontrar direcciones IP \n",
    "    ip_pattern = re.compile(r'^(\\d{1,3}\\.){3}\\d{1,3}') \n",
    "    return 1 if ip_pattern.match(url) else 0 \n",
    "#se añade has_ip\n",
    "datas['has_ip'] = datas['url'].apply(starts_with_ip)\n",
    "\n",
    "\n",
    "#movemos la columna 'status' al final de las columnas \n",
    "columns = [col for col in datas.columns if col != 'status'] + ['status'] \n",
    "datas =datas[columns]\n",
    "print(datas.head())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de URLs duplicadas: 7330\n",
      "verificando se tiene filas repetidas filtrado por ip\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#filtramos las urls con direccion ip\n",
    "ip_pattern = re.compile(r'^(\\d{1,3}\\.){3}\\d{1,3}')\n",
    "filtered = datas[datas['url'].str.match(ip_pattern)]\n",
    "\n",
    "# Contar el número total de duplicados \n",
    "duplicados = filtered[filtered.duplicated(subset='url', keep=False)]\n",
    "conteo_duplicados = duplicados['url'].value_counts()\n",
    "total_duplicados = conteo_duplicados.sum() \n",
    "print(f\"Total de URLs duplicadas: {total_duplicados}\")\n",
    "\n",
    "#eliminado duplicados del data\n",
    "datas_actualizado= datas.drop_duplicates(subset='url', keep='first')\n",
    "\n",
    "# actualizar el data original sin las urls duplicadas\n",
    "datos_actualizado= datas[~datas.index.isin(duplicados.index)]\n",
    "#verificamos q se han eliminado\n",
    "ip_pattern2 = re.compile(r'^(\\d{1,3}\\.){3}\\d{1,3}')\n",
    "filtered2 = datos_actualizado[datos_actualizado['url'].str.match(ip_pattern)]\n",
    "conteo_duplicados2= filtered2.duplicated(subset='url', keep=False).sum()\n",
    "print(\"verificando se tiene filas repetidas filtrado por ip\")\n",
    "print(conteo_duplicados2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_actualizado.to_csv('datos1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 943506 entries, 0 to 943505\n",
      "Columns: 103 entries, url to status\n",
      "dtypes: int64(102), object(1)\n",
      "memory usage: 741.4+ MB\n"
     ]
    }
   ],
   "source": [
    "datos= pd.read_csv('datos1.csv')\n",
    "datos.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
